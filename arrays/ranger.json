{"episode_rewards": [4.949999971315265, 8.889999998733401], "episode_timesteps": [72, 3], "mean_entropies": [2.9242330687524576, 2.926753412610804], "std_entropies": [0.003686919726881149, 0.0026502241678113907], "reward_model_loss": [], "env_rewards": []}