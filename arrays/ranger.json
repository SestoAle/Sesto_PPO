{"episode_rewards": [-3.4300000239163637, -4.8700000513345, -3.160000018775463, 6.689999973401427, 1.3499999884516], "episode_timesteps": [100, 100, 100, 89, 72], "mean_entropies": [-3.4300000239163637, -4.8700000513345, -3.160000018775463, 6.689999973401427, 1.3499999884516], "std_entropies": [-3.4300000239163637, -4.8700000513345, -3.160000018775463, 6.689999973401427, 1.3499999884516], "reward_model_loss": [], "env_rewards": []}