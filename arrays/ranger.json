{"episode_rewards": [-4.330000041052699, 8.53999999165535, -4.780000049620867, -3.0700000170618296, -1.140000006183982], "episode_timesteps": [100, 48, 100, 100, 61], "mean_entropies": [2.934650171850026, 2.936603946827612, 2.935985651616952, 2.934633967071322, 2.9367539706593115], "std_entropies": [0.0031920272858227254, 0.001905549513865206, 0.0026522960003377586, 0.0025558425846071036, 0.0019396117656837746], "reward_model_loss": [], "env_rewards": []}